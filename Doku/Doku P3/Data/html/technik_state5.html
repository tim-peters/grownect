	<div id="kopfleiste">
		<a href="./trailer.html"><img src="Bilder/Logo grownect.png" height="60" style="margin-left:20px"></a>
	</div>
	

<div id="infobox">
<ul>
	<li><strong>Weitere Details</strong></li>
	<br> 
	<li>Im Grunde ist die View auf dem Spiegel-Display mit einer Webseite gleichzusetzen.</li>
	<li><a href="./files/API%20Dokumentation.xlsx">Kommunikationsarchitektur</a></li>
	<li><a href="./files/uml.png">Klassenarchitektur (UML)</a></li>
	<li><a href="./files/DB_Struktur.txt">Datenbankaufbau</a></li>
	<li><a href="./files/Technische_Anforderungen_Devices.pdf">Auflistung benötigter Technik in den Devices</a><br></li>
	<li><a href="./files/verzögerer-kommunikation.jpg">Diagram Konfliktlösung: Erster Schritt</a></li>
</ul>
</div>


<div id="menu">
		<ul>
			<li><a href="trailer.html">Trailer</a></li>
			<li><a href="connect.html">Connect</a></li>
			<li><a href="recherche.html">Recherche</a></li>
			<li><a href="konzept.html">Konzept</a></li>
			<li><a href="interaktion.html">Interaktion</a></li>
			<li class="active"><a href="technik.html">Technik</a></li>
			<li><a href="design.html">Design</a></li>
			<li><a href="management.html">Management</a></li>
			<li><a href="kontakt.html">Kontakt</a></li>
		</ul>
	</div>

<div id="inhalt">
		<h2>SPRACHEINGABE/-AUSGABE</h2>

		<p>An verschiedenen Stellen des Systems, insbesondere auf Seiten des Armbands kommt Sprachein- und -Ausgabe zum Einsatz. Auch hier galt es, funktionierende Lösungen für den Prototyp zu finden, die so wenig Implementierungsaufwand wie möglich erforderten.
<br><br>
Für die Sprachausgabe (Text-to-Speech) nutzt das System wieder einen externen Service. Hierbei standen diverse Systeme zur Auswahl, in sich in ihrer Funktionalität stets ähnelten. Fast immer wird ein Text als String (ggf. zusammen mit weiteren Parametern) an eine API übermittelt, die dann eine Sounddatei zurück sendet.<br>
Die Impementierung der Sprachausgabe erfolgt hier stets vom Armband aus. Dieses erhält den auszugebenden Text und lässt diesen selbstständig in Sprache umwandeln, die es dann ausgibt.<br><br>
Eine Schwierigkeit lag hierbei darin, Überschneidungen verschiedener Ausgaben oder auch mit anderen Funktionen zu vermeiden. Somit musste ein komplett sequenzieller Aufbau mit asynkroner Kommunikation mit  teilweise mehrfach verschachtelten Callback-Funktionen gewählt werden.
<br><br>
Für die Spracheingabe bedient sich das System der in modernen Android Smartphones bereits integrierten Spracheingabe-Funktion mit Hilfe eines Google Services. Diese bietet die Möglichkeit, Spracheingabe anstatt einer Software-Tastatur zu benutzen und ist somit an jeder Stelle verfügbar, an der normalerweise Eingaben mit der Tastatur getätigt werden können. Stellt man die Spracheingabe auf dem Smartphone als Standard ein, und generiert ein einfaches, aktives Textfeld, so poppt auf dem Display automatisch die Spracheingabe-Funktion auf. Der Nutzer muss dies nurnoch bestätigen und anfangen zu sprechen. Per Javascript wird dann der diktierte Text ausgelesen und über die API an den Spiegel übertragen. <br><br>
Diese vereinfachte Art der Implementierung hat in so fern ihre Berechtigung, als dass die Funktionalität hiermit eins-zu-eins gezeigt werden kann (eine eigene, finale Implementierung würde auf die gleiche Voice-Recognition-Schnittstelle zugreifen), bei einem Bruchteil des Programmieraufwands. Um nur zu zeigen wie ein Konzept funktioniert und was machbar ist, ist sie also ideal.
		</p>
		<div class="buttoncentered">
 <a href="" ng-click="back()" ng-show="num > 0"> <button type="button" type="submit" class="btn2"><img src="bilder/back.png" width="25">Zurück</button></a>
  <a href="" ng-click="next()" ng-show="num<content.length-1"> <button type="button" type="submit" class="btn"><img src="bilder/confirm.png" width="25">Weiter</button></a>
</div>
</div>
