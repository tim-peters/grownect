<!DOCTYPE html>
<html>
<head>
	<meta charset="UTF-8">
	<title>grownect</title>
	<link rel="stylesheet" type="text/css" href="./style/stylesheet.css">

<script src="./js/jquery-1.10.2.js" type="text/javascript"></script>
<script type="text/javascript">


</script>
</head>
<body bgcolor="black";>

	<div id="kopfleiste">
		<img src="Bilder/Logo grownect.png" height="60" style="margin-left:20px">
	</div>
	

<div id="infobox">
<ul>
	<li><strong>Weitere Details</strong></li>
	<br> 
	<li> Zu Armband: Auflistung benötigter Sensoren aus Pad <br> <br>Im Grunde ist die View auf dem Spiegel-Display mit einer Webseite gleichzusetzen.
		<br></li> 
</ul>
</div>


<div id="menu">
		<ul>
			<li><a href="trailer.html">Trailer</a></li>
			<li><a href="connect.html">Connect</a></li>
			<li><a href="recherche.html">Recherche</a></li>
			<li><a href="konzept.html">Konzept</a></li>
			<li><a href="interaktion.html">Interaktion...</a></li>
			<li class="active"><a href="technik.html">Technik</a></li>
			<li><a href="design.html">Design</a></li>
			<li><a href="management.html">Management</a></li>
			<li><a href="kontakt.html">Kontakt</a></li>
		</ul>
	</div>


	<div id="inhalt">
		<h2>GRUNDSÄTZLICHE HERANGEHENSWEISE</h2>

		<p>Technische liegt der Fokus darauf, die Technik so unauffällig und zurückhaltend wie möglich zu halten. Der Nutzer soll einfach mit dem System interagieren können ohne das Gefühl zu haben eine komplizierte Maschine vor sich zu haben. Die Technik soll den Nutzer aktiv beeinflussen und beeinflusst werdem, aber in keinem Fall selbst im Mittelpunkt stehen.<br>
Dies macht die Entwicklung nicht etwa, wie man fälschlich denken könnte einfacher, sondern wesentlich anspruchsvoller. Sämtliche Funktionen müssen nämlich nicht nur zu einhundert Prozent funktional sein - etwaige Fehler würden den Nutzer ja wieder daran erinnern, dass er einen Computer vor sich hat - sondern auch die technische Infrastruktur muss sich so weit im Hintergrund halten, dass der Nutzer sie im Idealfall vergisst.</p>
<p>Um dies zu erreichen nutzen wird ein vielfach eingesetztes und erprobtes Grundsystem genutzt, dass so abgewandelt und angepasst ist, dass es unsere Anforderungen perfekt erfüllt - im Idealfall sowohl für den entstehenden Prototypen als auch darüber hinaus, für die etwaige Entwicklung eines finalen Produkts.<br>
Somit stand vor der eigentlichen Entwicklung eine ausführliche Phase der technischen Konzeption und Entwicklung einer geeigneten Architektur. Auch wenn diese Phase durchaus ausführlicher hätte ausfallen können, so hat sich das einfache Existieren übergeordneter Gedanken doch vielfach ausgezahlt und so mit Blick auf den gesamten Projektzeitraum wohl vielfach ausgezahlt.</p>
<p>Beim technischen Aufbau des Systems handelt es sich im Grunde um eine klassische 3-Tier-Architektur. Hierbei kommt ein Apache-Server zum Einsatz (bei unserem Prototypen hängt dieser direkt am Spiegel). Auf diesem lauft das eigentliche Programm, geschrieben in objektorientiertem und in weiten Teilen modular aufgebautem PHP. <br>
An den Server angebunden ist eine relationale Datenbank - hier kommt MySQL zum Einsatz - die die für die Arbeit des Servers nötigen Daten, aus denen dann die Views für die beiden Clients erzeugt werden liefert. Die Datenbankarchitektur ist hierbei so aufgebaut, dass Informationen so separiert wie möglich, aber trotzdem stets so nah und verknüpft wie nötig abgelegt sind. Dies soll beim späteren Betrieb ermöglichen, dass ein größtmöglicher Teil an Informationen herausgezogen werden kann, bei gleichzeitig kleinstmöglichem Aufwand.<br>
Als Clients im System können sowohl der Spiegel mit seiner View als auch das Bracelet, dass in unserem Prototyp durch ein Smartphone simuliert wird (hierzu später mehr) bezeichnet werden.
		</p>
		<div id="startbild">
		<img src="Bilder/kommunikation.png" alt="Start" width="670" ></div>
</div>

	<div id="inhalt">
		<h2>KOMMUNIKATION/EVENTBUSSES</h2>

		<p>Warum das System trotzdem streng genommen mehr als eine 3-Tier-Architektur ausmacht, zeigt sich beim Blick auf die Kommunikation der Devices. Diese ist entgegen der klassischen Server-Client-Beziehung zu jeder Zeit bi-direktional möglich. Dies bedeutet, dass nicht nur die Clients Requests an den Server senden können, der daraufhin anfängt zu arbeiten und (im Idealfall) mit dem Code zum anzeigen einer View antwortet, sondern dass auch der Server (ungefragt) Events auf Seite der beiden Clients auslösen kann. Ein solches Vorgehen gibt das http-Protokoll in seiner klassischen Form nicht her. Das System bedient sich hier dem Eventbus-Standard (websockets), ausgerüstet mit zahlreichen Fallbacks zur Gewährleistung der Kompatibilität auch mit alten Systemen. Im Prototyp ist dieser Schritt zur Gewährleistung des bestmöglichen Zeit-Nutzen-Verhältnisses an einen externen Dienstleister (pusher.com) ausgelagert. Dieser stellt die nötige Infrastruktur und eine Schnittstelle zur Verfügung.<br>
		Innerhalb des Systems ist die Kommunikation über den Eventbus sowohl logisch in die Klassenarchitektur integriert (wo sie innerhalb der normalen serverseitigen Berechnungen stattfindet) als auch an einer extra hierfür erstellten Stelle gebündelt. Diese Stelle empfängt zum einen die Antworten der Clients und wertet sie (anfänglich) aus, zum anderen sendet sie die Events aus, die sich unmittelbar aus Antworten von Devices ergeben. Über diesen API-Teil läuft somit ein Großteil der Client-Server-Kommunikation abseits von klassischen HTTP-Requests. Eine Übersicht der Kommunikationsarchitektur findet sich hier (FIXME: Link zur API-Tabelle).</p>
</div>


	<div id="inhalt">
		<h2>BILDER-UPLOAD</h2>

		<p>Ein Kernteil des Systems, der in seiner Art von der üblichen Client-Server-Kommunikation abweicht ist der Bilder-Upload. Hierbei kommt das einzige Mal auch konzeptionell ein weiteres Gerät (und damit ein weiterer Client) zu unserem System dazu.
Der Bilder-Upload taucht an verschiedenen Stellen des Systems, für verschiedene Einsatzzwecke auf. Seine Grundfunktionalität ist aber immer die gleiche. Somit bot sich ein modularer Aufbau an.
Der Bilder-Upload funktioniert stets über einen (serverseitig) generierten QR-Code. Über diesen wird eine URL übergeben, die eine einzigartige ID enthält, zu der ein gleichnamiges Verzeichnis angelegt wird. Scannt der Nutzer nun den QR-Code, folgt also dem Link, wird er auf eine Seite geleitet, auf der er (Bspw. von seinem Smartphone) gespeicherte oder auch grade fotografierte Bilder in das extra angelegte Verzeichnis hochladen kann. Parallel kommt auf der Seite auf der der QR-Code angezeigt wird ein JavaScript zum Einsatz, welches in Sekundenabständen bei der API anfragt, ob bereits ein Upload mit dieser ID passiert ist, ob also bereits ein Bild im genannten Verzeichnis liegt. Ist dies der Fall, wird der QR-Code durch das hochgeladene Bild ersetzt und der Pfad zum Bild in ein unsichtbares Formularfeld geschrieben. Der Nutzer kann das Ergebnis also nochmal auf dem Spiegel sehen, bevor er es mit dem Bestätigen-Button final einrichtet. Hier passiert dann auch erst die Verarbeitung des Bildes (konvertieren, beschneiden, verkleinern).
<br><br>
Später zeigte sich, dass anstatt der sich wiederholenden Nachfrage bei der API auch eine Erkennung eines Uploads über den Eventbus möglich gewesen wäre. Da die zu diesem Zeitpunkt bereits existierende Lösung aber zufriedenstellend und ohne störende Verzögerung oder zu großem Ressourcenverbrauch funktionierte, gab es keinen Grund diese vorerst nochmal umzustellen.
</p>
</div>


<div id="inhalt">
		<h2>ARMBAND</h2>

		<p>Für die technische Konzeption des Prototyps musste zuerst anhand des Konzepts eine Aufstellung aller benötigten Sensoren und technischen Komponenten erstellt werden. Hierbei zeigte sich schnell, dass eine Komplettumsetzung eines Armbands in der Kürze der Zeit wohl kaum möglich sein würde. Auch der ursprüngliche Plan die Technik auf Basis eines Klein- oder Einplatinencomputers (z.B. Raspberry Pi) umzusetzen und zusätzlich einen Design-Dummy per 3D-Druck zu erstellen, stellte sich schnell als zu ambitioniert heraus.
Letztendlich bietet die Unterbringung der Armband-Technik in einem Smartphone erhebliche Vorteile. So existiert nicht nur die gesamte technische Infrastruktur bereits - es musst nichts selbst zusammengelötet werden - sondern auch die softwareseitige Umsetzung wird durch das bestehende System, in diesem Fall Android erheblich vereinfacht. Gleichzeitig ist das System aber so nah an modernen Kleinstcomputern dran, dass mittelfristig eine Portierung auf ein System in einem Armband durchaus realistisch erscheint.

	</p>
</div>



<div id="inhalt">
		<h2>SPRACHEINGABE/-AUSGABE</h2>

		<p>An verschiedenen Stellen des Systems, insbesondere auf Seiten des Armbands kommt Sprachein- und -Ausgabe zum Einsatz. Auch hier galt es, funktionierende Lösungen für den Prototyp zu finden, die so wenig Implementierungsaufwand wie möglich erforderten.
<br><br>

Für die Sprachausgabe (Text-to-Speech) nutzt das System wieder einen externen Service. Hierbei standen diverse Systeme zur Auswahl, in sich in ihrer Funktionalität stets ähnelten. Fast immer wird ein Text als String (ggf. zusammen mit weiteren Parametern) an eine API übermittelt, die dann eine Sounddatei zurück sendet.<br>
Die Impementierung der Sprachausgabe erfolgt hier stets vom Armband aus. Dieses erhält den auszugebenden Text und lässt diesen selbstständig in Sprache umwandeln, die es dann ausgibt.<br><br>
Eine Schwierigkeit lag hierbei darin, Überschneidungen verschiedener Ausgaben oder auch mit anderen Funktionen zu vermeiden. Somit musste ein komplett sequenzieller Aufbau mit asynkroner Kommunikation mit  teilweise mehrfach verschachtelten Callback-Funktionen gewählt werden.
<br><br>
Für die Spracheingabe bedient sich das System der in modernen Android Smartphones bereits integrierten Spracheingabe-Funktion mit Hilfe eines Google Services. Diese bietet die Möglichkeit, Spracheingabe anstatt einer Software-Tastatur zu benutzen und ist somit an jeder Stelle verfügbar, an der normalerweise Eingaben mit der Tastatur getätigt werden können. Stellt man die Spracheingabe auf dem Smartphone als Standard ein, und generiert ein einfaches, aktives Textfeld, so poppt auf dem Display automatisch die Spracheingabe-Funktion auf. Der Nutzer muss dies nurnoch bestätigen und anfangen zu sprechen. Per Javascript wird dann der diktierte Text ausgelesen und über die API an den Spiegel übertragen. <br><br>
Diese vereinfachte Art der Implementierung hat in so fern ihre Berechtigung, als dass die Funktionalität hiermit eins-zu-eins gezeigt werden kann (eine eigene, finale Implementierung würde auf die gleiche Voice-Recognition-Schnittstelle zugreifen), bei einem Bruchteil des Programmieraufwands. Um nur zu zeigen wie ein Konzept funktioniert und was machbar ist, ist sie also ideal.
		</p>
</div>

<div id="inhalt">
<h2>SPRACHEINGABE/-AUSGABE</h2>
<p>An verschiedenen Stellen des Systems, insbesondere auf Seiten des Armbands kommt Sprachein- und -Ausgabe zum Einsatz. Auch hier galt es, funktionierende Lösungen für den Prototyp zu finden, die so wenig Implementierungsaufwand wie möglich erforderten.</p>
<p>Für die Sprachausgabe (Text-to-Speech) nutzt das System wieder einen externen Service. Hierbei standen diverse Systeme zur Auswahl, in sich in ihrer Funktionalität stets ähnelten. Fast immer wird ein Text als String (ggf. zusammen mit weiteren Parametern) an eine API übermittelt, die dann eine Sounddatei zurück sendet.<br>
Die Impementierung der Sprachausgabe erfolgt hier stets vom Armband aus. Dieses erhält den auszugebenden Text und lässt diesen selbstständig in Sprache umwandeln, die es dann ausgibt.<br>
Eine Schwierigkeit lag hierbei darin, Überschneidungen verschiedener Ausgaben oder auch mit anderen Funktionen zu vermeiden. Somit musste ein komplett sequenzieller Aufbau mit asynkroner Kommunikation mit  teilweise mehrfach verschachtelten Callback-Funktionen gewählt werden.</p>
<p>Für die Spracheingabe bedient sich das System der in modernen Android Smartphones bereits integrierten Spracheingabe-Funktion mit Hilfe eines Google Services. Diese bietet die Möglichkeit, Spracheingabe anstatt einer Software-Tastatur zu benutzen und ist somit an jeder Stelle verfügbar, an der normalerweise Eingaben mit der Tastatur getätigt werden können. Stellt man die Spracheingabe auf dem Smartphone als Standard ein, und generiert ein einfaches, aktives Textfeld, so poppt auf dem Display automatisch die Spracheingabe-Funktion auf. Der Nutzer muss dies nurnoch bestätigen und anfangen zu sprechen. Per Javascript wird dann der diktierte Text ausgelesen und über die API an den Spiegel übertragen.
Diese vereinfachte Art der Implementierung hat in so fern ihre Berechtigung, als dass die Funktionalität hiermit eins-zu-eins gezeigt werden kann (eine eigene, finale Implementierung würde auf die gleiche Voice-Recognition-Schnittstelle zugreifen), bei einem Bruchteil des Programmieraufwands. Um nur zu zeigen wie ein Konzept funktioniert und was machbar ist, ist sie also ideal.</p>
</div>

<div id="inhalt">
		<h2>KONZEPT SMARTHOME DEVICES</h2>

		<p>Wie im konzeptionellen Teil bereits angesprochen, ist eine Erweiterung des Systems um eine Event-Tracking Funktion mittelfristig denkbar. Hierbei soll das System eigenständig verschiedene Situationen erkennen. Diese reichen von einfachen Software-Events (“Der Nutzer hat einen Termin in seinem Kalender der jetzt anfängt”), über Events auf Basis der systemeigenen Sensoren (“Der Nutzer befindet sich an der Positon X,Y”) bis hin zu Events, die von externen Geräten und Sensoren erkannt werden müssen (“Der Nutzer hat die Milch leer gemacht”). <br><br>
Um diese letzte Gruppe von Events tracken zu können, muss das System mit verschiedenen anderen Geräten kommunizieren können. Konzeptionell sind hier sowohl eigene Geräte auf Basis von Kleinstcomputern, wie z.B. einem Toilettenpapier-Halter, der erkennt, ob eine leere oder volle Rolle Toilerrenpapier aufgehängt ist als auch die Kommunikation mit Smart-Home-Geräten angedacht. So soll zukünftig beispielsweise erkannt werden können, ob der Nutzer beim Verlassen des Wohnung (getrackt per GPS oder schlicht durch Verlassen des heimischen Wlans) das Licht ausgeschaltet hat oder nicht. Wichtig ist, dass die externen Geräte stets nur die nötigen Informationen und Events liefern. Eine Auswertung erfolgt ausschließlich auf dem Systemeigenen Server. <br>
Auch wenn diese Funktion im Rahmen des Prototyps nicht voll umgesetzt ist, so ist sie in der Software-Architektur doch bedacht und sind Grundstrukturen angelegt.
		</p>
</div>

<div id="inhalt">
<h2>SPIEGEL HARDWARE</h2>
<p>Die technische Entwicklung der Spiegel Hardware war stets von den zu erfüllenden Zielen und den damit einhergehenden Vorraussetzungen getrieben. So war schnell klar, dass die Liste des zu Erfüllenden eine touchsensitive Fläche, genau wie eine einseitig Lichtdurchlässige Scheibe und einen hochauflösenden Bildschirm beinhaltete. Auf Grund eines angenommenen Mangels professioneller Hardware, entstanden aber zahlreiche Pläne, die auch ohne alle Komponenten des System in einer fertigen Version auskämen. So bestand beispielsweise anfangs der Plan, die Umsetzung des Touchscreens in Ermangelung eines echten Touchscreens anfangs mit einer Infrarotkamera (in einer Wii-Fernbedienung), eines sogar extra angefertigten Infrarotstifts und der passenden Software durchzuführen. Auch wurden Alternativen, die ohne hochauflösenden Bildschirm auskämen erdacht.</p>
<p>Letztendlich ist der Spiegel aber wie folgt aufgebaut:</p>
<ul>
<li><p>In der hintersten Schicht liefert ein lichtstarker, hochkant montierter FullHD-Screen das benötigte Bild.
Dieses wird durch eine Glasscheibe mit darauf aufgebrachter Einweg-Spiegelfolie gesendet. Durch die Folie erhält die Scheibe die Eigenschaft, stets von der helleren Seite aus verspiegelt und von der dunkleren aus lichtdurchlässig zu sein. Dies soll dazu führen, dass der Nutzer sich in dunklen Flächen oder bei ausgeschaltetem Bildschirm optimal speigeln kann. Leuten aber Teile des Bildschirms, wird die Scheibe an dieser Stelle klar und das Bild ist frei erkennbar.</p></li>
<li><p>Auf der Glasscheibe ist nun ein professioneller Touch-Rahmen montiert, der Bewegungen auf der Scheibe per Infrarot erkennt und als Maus- bzw. Toucheingabe an den angeschlossenen Rechner weiterleitet.</p></li>
<li><p>Konzeptionell ist hiervor noch eine weitere Schicht vorgesehen, die sich elektronischen eintrüben lässt. Dies ist konzeptionell als “Ultima Ration”-Aktion vorgesehen und soll den Blick in den Spiegel verhindern.</p></li>
<li><p>Im Prototyp ist dieser Screen durch einen Blitz ausgefüllt, der durch seine große helle Fläche auch ein ungehindertes Begutachten im Spiegel verhindert.</p></li>
</ul>
</div>
	
	<div id="inhalt">
		<h2>MIRROR</h2>

		<p>

		</p>
</div>

	
<div class="buttoncentered">
		 <a href="interaktion.html"> <button type="button" type="submit" class="btn2"><img src="bilder/back.png" width="25">Zurück</button></a>
		  <a href="design.html"> <button type="button" type="submit" class="btn"><img src="bilder/confirm.png" width="25">Weiter</button></a>
</div>

</body>
</html>
